# Optimizer-Study
This is for beginners starting ML like me, can follow this to have better understanding in Optimizers used in Neural Networks.  
## What is optimizers?
Optimizers are used to update weights and biases i.e. the internal parameters of a model to reduce the error. The most important technique and the foundation of how we train and optimize our model is using Gradient Descent.

## Types of Optimizers
1. Gradient Descent (GD)
2. Stochastic Gradient Descent
3. Momentum Based Gradient Descent
4. Adagrad
5. RMSProp
6. Adam

## Content
1. In **Optimizer_Study.pdf** file I have explained how all popular optimizers work.I have also made comparision between them through which you can make decision which one should be selected for your algorithm.
2. The **Optimizer_Study.ipynb** file is the implementation of the same, you can observe their behaviour on a given prediction model i.e. on a "House Price" prediction.
3. The **houseprice.csv** file the a data used for prediction model. You are encouraged to upload the same when you are using the code.
